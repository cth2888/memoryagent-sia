<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="description"
        content="Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles" />
    <meta name="keywords, puzzle, logical, reasoning, large lanuge model, LLM"
        content="Enigmata" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />
    <link rel="stylesheet" href="./css/bulma.min.css" />
    <link rel="stylesheet" href="./css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="./css/bulma-slider.min.css" />
    <link rel="stylesheet" href="./css/fontawesome.all.min.css" />
    <!-- <link rel="stylesheet" href="./css/fontawesome_6_7_2.all.css" /> -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
    <link rel="stylesheet" href="./css/index.css" />
    <link rel="stylesheet" href="./css/enhanced-styles.css" />
    <link rel="stylesheet" href="./css/final-enhancements.css" />
    <link rel="stylesheet" href="./css/navbar-fix.css" />
    <link rel="stylesheet" href="./css/navbar-alignment-fix.css" />
    <link rel="stylesheet" href="./css/overlap-fix.css" />
    <link rel="stylesheet" href="./css/spacing-fix.css" />
    <link rel="stylesheet" href="./css/author-affiliation-styles.css" />
    <link rel="icon" href="./assets/doubao.png" type="image/png" />
    <!-- add page icon at 64 x 64-->
    <!-- <link rel="icon" type="image/png" href="./assets/re.png" sizes="256x256" /> -->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./js/fontawesome.all.min.js"></script>
    <script src="./js/bulma-carousel.min.js"></script>
    <script src="./js/bulma-slider.min.js"></script>
    <script src="./js/index.js"></script>
    <script src="./js/enhanced-animations.js"></script>
    <script src="./js/language-switcher.js"></script>
    <script src="https://cdn.tailwindcss.com"></script>
</head>

<body>
    <!-- Enhanced Navigation Bar -->
    <nav class="navbar is-fixed-top" role="navigation" aria-label="main navigation">
        <div class="container">
            <div class="navbar-brand">
                <a class="navbar-item" href="#">
                    🧩 Enigmata
                </a>
                <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                </a>
            </div>
            <div class="navbar-menu">
                <div class="navbar-end">
                    <a class="navbar-item" href="#introduction">
                        <span lang="en">Introduction</span>
                        <span lang="zh">介绍</span>
                    </a>
                    <a class="navbar-item" href="#data">
                        <span lang="en">Data</span>
                        <span lang="zh">数据</span>
                    </a>
                    <a class="navbar-item" href="#eval">
                        <span lang="en">Evaluation</span>
                        <span lang="zh">评估</span>
                    </a>
                    <a class="navbar-item" href="#model">
                        <span lang="en">Model</span>
                        <span lang="zh">模型</span>
                    </a>
                    <a class="navbar-item" href="#citation">
                        <span lang="en">Citation</span>
                        <span lang="zh">引用</span>
                    </a>
                    <button id="language-toggle" class="navbar-item language-toggle">中文</button>
                </div>
            </div>
        </div>
    </nav>
    
    <section class="hero">
        <div class="hero-body">
            <div class="container">
                <div class="has-text-centered">
                    <h1 class="publication-title">
                        <span lang="en"><em class="dnerf">Enigmata</em>: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles</span>
                        <span lang="zh"><em class="dnerf">Enigmata</em>: 通过合成可验证谜题扩展大语言模型的逻辑推理能力</span>
                    </h1>
                    <div class="publication-authors">
                        <span class="author-block">
                            <span lang="en"><a href="https://seed-enigmata.github.io/">Hongli Yu</a><sup>1,3,6</sup></span>
                            <span lang="zh"><a href="https://seed-enigmata.github.io/">于鸿利</a><sup>1,3,6</sup></span>
                        </span>
                        <span class="author-block">
                            <span lang="en"><a href="https://scholar.google.com/citations?user=DDRBbxgAAAAJ&hl=zh-CN">Xuefeng Li</a><sup>1,5</sup></span>
                            <span lang="zh"><a href="https://scholar.google.com/citations?user=DDRBbxgAAAAJ&hl=zh-CN">李学峰</a><sup>1,5</sup></span>
                        </span>
                        <span class="author-block">
                            <span lang="en"><a href="https://scholar.google.com/citations?user=Vt1j3kEAAAAJ&hl=zh-CN">Jiaze Chen</a><sup>1</sup></span>
                            <span lang="zh"><a href="https://scholar.google.com/citations?user=Vt1j3kEAAAAJ&hl=zh-CN">陈家泽</a><sup>1</sup></span>
                        </span>
                        <span class="author-block">
                            <span lang="en"><a href="https://zhouh.github.io/">Hao Zhou</a><sup>3,6</sup></span>
                            <span lang="zh"><a href="https://zhouh.github.io/">周浩</a><sup>3,6</sup></span>
                        </span>
                        <span class="author-block">
                            <span lang="en"><a href="https://mingxuan.github.io/">Mingxuan Wang</a><sup>1,6</sup></span>
                            <span lang="zh"><a href="https://mingxuan.github.io/">王明轩</a><sup>1,6</sup></span>
                        </span>
                    </div>
                    <div class="publication-affiliations">
                        <span class="affiliation-block">
                            <span lang="en"><sup>1</sup>ByteDance Seed</span>
                            <span lang="zh"><sup>1</sup>字节跳动 Seed</span>
                        </span><br>
                        <span class="affiliation-block">
                            <span lang="en"><sup>3</sup>Institute for AI Industry Research (AIR), Tsinghua University</span>
                            <span lang="zh"><sup>3</sup>清华大学 智能产业研究院（AIR）</span>
                        </span><br>
                        <span class="affiliation-block">
                            <span lang="en"><sup>6</sup>SIA-Lab of Tsinghua AIR and ByteDance Seed</span>
                            <span lang="zh"><sup>6</sup>清华-字节联合实验室SIA-Lab</span>
                        </span>
                        <div class="affiliation-note">
                            <span lang="en">*Project Lead; †Equal Contribution</span>
                            <span lang="zh">*项目负责人; †同等贡献</span>
                        </div>
                        <div class="affiliation-note">
                            <span lang="en">Contact: <a href="mailto:jiangjiec@bytedance.com">jiangjiec@bytedance.com</a></span>
                            <span lang="zh">联系方式: <a href="mailto:jiangjiec@bytedance.com">jiangjiec@bytedance.com</a></span>
                        </div>
                    </div>
                    <div class="publication-links">
                        <span class="link-block">
                            <a href="http://arxiv.org/abs/2505.19914" class="external-link button is-dark">
                                <span class="icon">
                                    <i class="fas fa-file-pdf"></i>
                                </span>
                                <span lang="en">Paper</span>
                                <span lang="zh">论文</span>
                            </a>
                        </span>
                        <span class="link-block">
                            <a href="https://github.com/BytedTsinghua-SIA/Enigmata" class="external-link button is-dark">
                                <span class="icon">
                                    <i class="fas fa-database"></i>
                                </span>
                                <span lang="en">Code</span>
                                <span lang="zh">代码</span>
                            </a>
                        </span>
                        <span class="link-block">
                            <a href="https://huggingface.co/datasets/BytedTsinghua-SIA/Enigmata-Eval" class="external-link button is-dark">
                                <span class="icon">
                                    <i class="fas fa-database"></i>
                                </span>
                                <span lang="en">Evaluation</span>
                                <span lang="zh">评估</span>
                            </a>
                        </span>
                        <span class="link-block">
                            <a href="https://huggingface.co/BytedTsinghua-SIA/Enigmata-Qwen2.5-32B" class="external-link button is-dark">
                                <span class="icon">
                                    🤗
                                </span>
                                <span lang="en">Model</span>
                                <span lang="zh">模型</span>
                            </a>
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </section>

<section class="section" id="introduction">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-10">
                <h2 class="title is-3">
                    <span lang="en">Introduction</span>
                    <span lang="zh">介绍</span>
                </h2>
                <div class="content">

                    <p lang="en">
                        We present <strong>RL-Memory Agent</strong>, a novel framework for long-text processing that directly optimizes task performance through end-to-end reinforcement learning, without any architectural modifications to the base model.
                    </p>
                    <p lang="zh">
                        我们推出了<strong>RL-Memory Agent</strong>，这是一个全新的长文本处理框架，能够通过端到端的强化学习直接优化长文本任务性能，而无需更改底层模型架构。
                    </p>
                    <div
                        style="background-color: #f8fafc; border-radius: 12px; border-left: 6px solid #3a76ed; padding: 1.2rem; margin-bottom: 1.5rem;">
                            <div style="display: flex; flex-direction: column; gap: 0.8rem;">
                                <div style="display: flex; align-items: flex-start; gap: 0.8rem;"> 
                                    <div
                                        style="background-color: #3a76ed; color: white; padding: 0.4rem; border-radius: 50%; font-size: 0.9rem; display: flex; align-items: center; justify-content: center; min-width: 2rem; height: 2rem;">
                                        1
                                    </div>
                                    <div>
                                        <p style="margin: 0; line-height: 1.4;" lang="en"><strong>Novel memory mechanism: </strong>
                                            It introduces a notebook-style workflow where the agent reads input in segments and efficiently updates memory using an overwrite strategy. This enables handling virtually unlimited-length inputs within a fixed context window, fundamentally breaking the hard length limits of Transformer models.</p>
                                        <p style="margin: 0; line-height: 1.4;" lang="zh"><strong>新型记忆机制：</strong>
                                            我们引入了一种记事本式的工作流，Agent 以分段方式读取文本，并借助覆写策略高效更新记忆。该设计使得模型能够在固定上下文窗口内处理几乎任意长度的输入，从根本上突破了传统 Transformer 架构的窗口长度限制。</p>
                                    </div>
                                </div>
                        
                                <div style="display: flex; align-items: flex-start; gap: 0.8rem;">
                                    <div
                                        style="background-color: #3a76ed; color: white; padding: 0.4rem; border-radius: 50%; font-size: 0.9rem; display: flex; align-items: center; justify-content: center; min-width: 2rem; height: 2rem;">
                                        2
                                    </div>
                                    <div>
                                    <p style="margin: 0; line-height: 1.4;" lang="en"><strong>O(n) linear complexity:</strong> 
                                        The agent design decouples compute and memory cost from input length, reducing the processing burden from quadratic to linear with respect to input size.</p>
                                    <p style="margin: 0; line-height: 1.4;" lang="zh"><strong>O(n) 线性复杂度：</strong>
                                        这一创新性的 Agent 设计将计算与显存开销与文本长度解耦，使得处理长文本的复杂度由原本的二次方增长转变为线性增长。</p>
                                    </div>
                                </div>

                                <div style="display: flex; align-items: flex-start; gap: 0.8rem;">
                                    <div
                                        style="background-color: #3a76ed; color: white; padding: 0.4rem; border-radius: 50%; font-size: 0.9rem; display: flex; align-items: center; justify-content: center; min-width: 2rem; height: 2rem;">
                                        3
                                    </div>
                                    <div>
                                        <p style="margin: 0; line-height: 1.4;" lang="en"><strong>RL-driven extrapolation:</strong>
                                            A modified GRPO algorithm supports training on multi-turn generation tasks with independent contexts. As a result, models trained with 32K contexts can extrapolate to 3.5M-token sequences with <5% performance drop.</p>
                                        <p style="margin: 0; line-height: 1.4;" lang="zh"><strong>强化学习驱动的外推能力:</strong>
                                            我们改进了 GRPO 算法，使其支持独立上下文的多轮生成训练。基于此，训练出的 Memory Agent 在32K上下文长度的数据上训练后，能够无损外推至3.5M token的问答任务中，性能下降低于5%。</p>
                                    </div>
                                </div>                                
                        </div>
                    </div>
                    <p lang="en">
                        <strong>RL-Memory Agent</strong> achieves three major breakthroughs:
                        <br><br>
                        - <em>Novel memory mechanism:</em> It introduces a notebook-style workflow where the agent reads input in segments and efficiently updates memory using an overwrite strategy. This enables handling virtually unlimited-length inputs within a fixed context window, fundamentally breaking the hard length limits of Transformer models.
                        <br>
                        - <em>O(n) linear complexity:</em> The agent design decouples compute and memory cost from input length, reducing the processing burden from quadratic to linear with respect to input size.
                        <br>
                        - <em>RL-driven extrapolation:</em> A modified GRPO algorithm supports training on multi-turn generation tasks with independent contexts. As a result, models trained with 32K contexts can extrapolate to 3.5M-token sequences with <5% performance drop.
                    </p>

                    <p lang="zh">
                        <strong>RL-Memory Agent</strong> 实现了三大核心突破：
                        <br><br>
                        - <em>新型记忆机制：</em> 我们引入了一种记事本式的工作流，Agent 以分段方式读取文本，并借助覆写策略高效更新记忆。该设计使得模型能够在固定上下文窗口内处理几乎任意长度的输入，从根本上突破了传统 Transformer 架构的窗口长度限制。
                        <br>
                        - <em>O(n) 线性复杂度：</em> 这一创新性的 Agent 设计将计算与显存开销与文本长度解耦，使得处理长文本的复杂度由原本的二次方增长转变为线性增长。
                        <br>
                        - <em>强化学习驱动的外推能力：</em> 我们改进了 GRPO 算法，使其支持独立上下文的多轮生成训练。基于此，训练出的 Memory Agent 在32K上下文长度的数据上训练后，能够无损外推至3.5M token的问答任务中，性能下降低于5%。
                    </p>

                    <p lang="en">
                        In a simple yet effective way, we demonstrate—for the first time—a trainable memory ability empowered by reinforcement learning. RL-Memory Agent showcases the untapped potential of optimizing specialized workflows via policy learning rather than architectural changes.
                    </p>
                    <p lang="zh">
                        我们以一种简洁而高效的方式，首次实现了真正意义上的、由强化学习赋予的可训练记忆能力，充分展现了强化学习在优化特定工作流方面的巨大潜力。
                    </p>

                    <strong><em style="color: #3a76ed"><center lang="en">We believe RL-Memory Agent paves the way for the next generation of scalable long-context LLMs.</center></em></strong>
                    <strong><em style="color: #3a76ed"><center lang="zh">我们相信，RL-Memory Agent 为下一代可扩展的长上下文大语言模型开辟了新路径。</center></em></strong>

                </div>
            </div>
        </div>
    </div>
</section>


    <section class="section" id="data">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-10">
                    <h2 class="title is-3">
                        <span lang="en">🧩 Enigmata-Data: Synthetic Verifiable Puzzle Generation</span>
                        <span lang="zh">🧩 Enigmata-Data: 合成可验证谜题生成</span>
                    </h2>
                    <div class="content">
                        <p lang="en"><strong>Enigmata-Data</strong> comprises <strong>36 distinct task types spanning 7 categories of logical reasoning puzzles</strong>. Each task is built with two core components: (1) <strong>Generator</strong>: Produces massive puzzle instances with precisely controllable difficulty parameters; (2) <strong>Verifier</strong>: Provides automatic, rule-based solution validation for reliable evaluation.</p>
                        <p lang="zh"><strong>Enigmata-Data</strong>包含<strong>跨越7个逻辑推理谜题类别的36种不同任务类型</strong>。每个任务都由两个核心组件构建：(1) <strong>生成器</strong>：生成具有精确可控难度参数的大量谜题实例；(2) <strong>验证器</strong>：提供自动的、基于规则的解决方案验证，以进行可靠的评估。</p>
                        <!-- <p>For comprehensive details on each task, please refer to our task documentation.</p> -->
                    </div>
                    <div
                        style="background-color: #f8fafc; border-radius: 12px; border-left: 6px solid #3a76ed; padding: 1.2rem; margin-bottom: 1.5rem;">
                            <div style="display: flex; flex-direction: column; gap: 0.8rem;">
                                <div style="display: flex; align-items: flex-start; gap: 0.8rem;"> 
                                    <div
                                        style="background-color: #3a76ed; color: white; padding: 0.4rem; border-radius: 50%; font-size: 0.9rem; display: flex; align-items: center; justify-content: center; min-width: 2rem; height: 2rem;">
                                        1
                                    </div>
                                    <div>
                                        <p style="margin: 0; line-height: 1.4;" lang="en"><strong>Unlimited Self-Verifying Data: </strong>
                                            It can generate an unlimited supply of self-verifying puzzle prompts, which plug seamlessly into the RLVR framework and support long chain-of-thought training.</p>
                                        <p style="margin: 0; line-height: 1.4;" lang="zh"><strong>无限自验证数据: </strong>
                                            它可以生成无限供应的自验证谜题提示，无缝插入RLVR框架并支持长思维链训练。</p>
                                    </div>
                                </div>
                        
                                <div style="display: flex; align-items: flex-start; gap: 0.8rem;">
                                    <div
                                        style="background-color: #3a76ed; color: white; padding: 0.4rem; border-radius: 50%; font-size: 0.9rem; display: flex; align-items: center; justify-content: center; min-width: 2rem; height: 2rem;">
                                        2
                                    </div>
                                    <div>
                                    <p style="margin: 0; line-height: 1.4;" lang="en"><strong>Controlled Difficulty:</strong> Programmatic difficulty control allows researchers to mix puzzles in desired difficulty ratios and to conduct fine-grained experiments on how curriculum design influences reinforcement learning.</p>
                                    <p style="margin: 0; line-height: 1.4;" lang="zh"><strong>可控难度:</strong> 程序化难度控制允许研究人员按照所需的难度比例混合谜题，并对课程设计如何影响强化学习进行细粒度实验。</p>
                                    </div>
                                </div>

                                <div style="display: flex; align-items: flex-start; gap: 0.8rem;">
                                    <div
                                        style="background-color: #3a76ed; color: white; padding: 0.4rem; border-radius: 50%; font-size: 0.9rem; display: flex; align-items: center; justify-content: center; min-width: 2rem; height: 2rem;">
                                        3
                                    </div>
                                    <div>
                                        <p style="margin: 0; line-height: 1.4;" lang="en"><strong>Flexible Task Sampling:</strong> Generators can emit arbitrary sample counts per task, enabling studies of task balancing and cross-task generalization.</p>
                                        <p style="margin: 0; line-height: 1.4;" lang="zh"><strong>灵活的任务采样:</strong> 生成器可以为每个任务发出任意数量的样本，从而能够研究任务平衡和跨任务泛化。</p>
                                    </div>
                                </div>                                
                        </div>
                    </div>
         
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="eval">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-10">
                    <h2 class="title is-3">
                        <span lang="en">⚖️ Enigmata-Eval: Evaluating Logical Reasoning Capabilities</span>
                        <span lang="zh">⚖️ Enigmata-Eval: 评估逻辑推理能力</span>
                    </h2>
                    <div class="content">
                        <p lang="en">Enigmata-Eval is a comprehensive benchmark containing 4,758 puzzle instances across Easy, Medium, and Hard difficulty levels. Each task provides 50 instances per difficulty level where possible, with strict train-eval separation to prevent data leakage.</p>
                        <p lang="zh">Enigmata-Eval是一个综合基准测试，包含4,758个跨越简单、中等和困难难度级别的谜题实例。每个任务尽可能提供每个难度级别50个实例，并严格分离训练和评估数据以防止数据泄漏。</p>

                        <p lang="en">📥 <strong>Download Enigmata-Eval</strong>: <a href="https://huggingface.co/datasets/BytedTsinghua-SIA/Enigmata-Eval">HuggingFace Dataset</a></p>
                        <p lang="zh">📥 <strong>下载Enigmata-Eval</strong>: <a href="https://huggingface.co/datasets/BytedTsinghua-SIA/Enigmata-Eval">HuggingFace数据集</a></p>
                    
         

                
                
                
                </div>
            </div>
        </div>
    </section>
    
    <section class="section" id="model">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-10">
                    <h2 class="title is-3">
                        <span lang="en">🤖 Enigmata-Model: The Training Recipe</span>
                        <span lang="zh">🤖 Enigmata-Model: 训练方法</span>
                    </h2>
                    <div class="content">
                        <p lang="en">Our training methodology follows a two-stage process designed to systematically build reasoning abilities: 
                            (1) rejection fine-tuning to establish foundational reasoning patterns, and 
                            (2) multi-task RL to develop general reasoning skills that transfer across diverse problem domains.</p>
                        <p lang="zh">我们的训练方法遵循两阶段过程，旨在系统地构建推理能力：
                            (1) 拒绝式微调以建立基础推理模式，以及
                            (2) 多任务强化学习，以发展可在不同问题领域之间迁移的通用推理技能。</p>
                    </div>
                    <div class="content">
                        <div
                        style="background-color: #f8fafc; border-radius: 12px; border-left: 6px solid #3a76ed; padding: 1.2rem; margin-bottom: 1.5rem;">
                            <div style="display: flex; flex-direction: column; gap: 0.8rem;">
                                <div style="display: flex; align-items: flex-start; gap: 0.8rem;"> 
                                    <div
                                        style="background-color: #3a76ed; color: white; padding: 0.4rem; border-radius: 50%; font-size: 0.9rem; display: flex; align-items: center; justify-content: center; min-width: 2rem; height: 2rem;">
                                        1
                                    </div>
                                    <div>
                                        <p style="margin: 0; line-height: 1.4;" lang="en"><strong>Rejection Fine-tuning:</strong>
                                            This initial stage focuses on building foundational reasoning by fine-tuning the model with high-quality solutions from a balanced mix of math and puzzle problems, including ARC-AGI.</p>
                                        <p style="margin: 0; line-height: 1.4;" lang="zh"><strong>拒绝式微调:</strong>
                                            这一初始阶段专注于通过使用来自数学和谜题问题（包括ARC-AGI）的平衡混合的高质量解决方案对模型进行微调，从而建立基础推理能力。</p>
                                    </div>
                                </div>
                        
                                <div style="display: flex; align-items: flex-start; gap: 0.8rem;">
                                    <div
                                        style="background-color: #3a76ed; color: white; padding: 0.4rem; border-radius: 50%; font-size: 0.9rem; display: flex; align-items: center; justify-content: center; min-width: 2rem; height: 2rem;">
                                        2
                                    </div>
                                    <div>
                                    <p style="margin: 0; line-height: 1.4;" lang="en"><strong>RL with Verifiable Puzzles:</strong> The model then undergoes reinforcement learning using VC-PPO, where an automated verifier for puzzles provides immediate rewards, enabling an automatic RL pipeline for puzzle reasoning.</p>
                                    <p style="margin: 0; line-height: 1.4;" lang="zh"><strong>可验证谜题的强化学习:</strong> 然后，模型使用VC-PPO进行强化学习，其中谜题的自动验证器提供即时奖励，从而为谜题推理启用自动强化学习流程。</p>
                                    </div>
                                </div>

                                <div style="display: flex; align-items: flex-start; gap: 0.8rem;">
                                    <div
                                        style="background-color: #3a76ed; color: white; padding: 0.4rem; border-radius: 50%; font-size: 0.9rem; display: flex; align-items: center; justify-content: center; min-width: 2rem; height: 2rem;">
                                        3
                                    </div>
                                    <div>
                                        <p style="margin: 0; line-height: 1.4;" lang="en"><strong>Multi-task Training:</strong> To develop general and transferable logical reasoning, the training incorporates multi-task methods like Mix-training RL and Multi-stage RL, combining diverse puzzle types (Enigmata, ARC-AGI) with challenging mathematical problems (AIME) while maintaining a balanced ratio.</p>
                                        <p style="margin: 0; line-height: 1.4;" lang="zh"><strong>多任务训练:</strong> 为了发展通用和可迁移的逻辑推理，训练结合了多任务方法，如混合训练强化学习和多阶段强化学习，将各种谜题类型（Enigmata、ARC-AGI）与具有挑战性的数学问题（AIME）结合起来，同时保持平衡比例。</p>
                                    </div>
                                </div>                                
                        </div>
                    </div>
                    </div>

                    
                    <div class="content">
                        <h4>
                            <span lang="en">👀 Experimental Results</span>
                            <span lang="zh">👀 实验结果</span>
                        </h4>
                        <div class="content">
                            <p lang="en">Our model, specifically the 32B parameter version, significantly outperforms most public models on Enigmata-Eval and ARC-AGI, showcasing enhanced general logical reasoning. This success stems from effective rejection fine-tuning (RFT) and multi-task RL strategies, which improve generalization while preserving existing math reasoning abilities.</p>
                            <p lang="zh">我们的模型，特别是32B参数版本，在Enigmata-Eval和ARC-AGI上的表现显著优于大多数公开模型，展示了增强的通用逻辑推理能力。这一成功源于有效的拒绝式微调（RFT）和多任务强化学习策略，它们在保留现有数学推理能力的同时提高了泛化能力。</p>
                        </div>
                        <figure>
                            <img src="./assets/main_results_1.png" alt="A descriptive alt text for your image" style="width: 600px;">
                            <figcaption style="color: gray;">
                                <span lang="en">Performance of reasoning, generic, and our trained LLMs on reasoning benchmarks</span>
                                <span lang="zh">推理型、通用型和我们训练的大语言模型在推理基准测试上的表现</span>
                            </figcaption>
                          </figure>
                        
                          <div class="content">
                            <p lang="en">On Enigmata-Eval, our Qwen2.5-32B-Enigmata model excels in Crypto, Arithmetic, and Logic tasks, indicating strong rule-based reasoning. It also performs competitively in search tasks, though spatial and sequential categories remain challenging.</p>
                            <p lang="zh">在Enigmata-Eval上，我们的Qwen2.5-32B-Enigmata模型在加密、算术和逻辑任务中表现出色，表明其具有强大的基于规则的推理能力。它在搜索任务中也表现得很有竞争力，尽管空间和序列类别仍然具有挑战性。</p>
                        </div>

                          <figure>
                            <img src="./assets/main_results2.png" alt="A descriptive alt text for your image" style="width: 600px;">
                            <figcaption style="color: gray;">
                                <span lang="en">Performance of reasoning LLMs, generic LLMs, and our trained LLMs on Enigmata-Eval</span>
                                <span lang="zh">推理型大语言模型、通用型大语言模型和我们训练的大语言模型在Enigmata-Eval上的表现</span>
                            </figcaption>
                          </figure>

                    </div>

                    
                    <div class="content">
                        <h4>
                            <span lang="en">🌟 Generalization with Scaling: Free Lunch from Enigmata</span>
                            <span lang="zh">🌟 随规模扩展的泛化能力：Enigmata的免费午餐</span>
                        </h4>
                    <div class="content">
                        <p lang="en">Incorporating the <strong>Enigmata-Data</strong> synthetic puzzle dataset into large-scale model training, e.g., <a href="https://arxiv.org/abs/2504.13914">Seed1.5-Thinking</a>, surprisingly improving performance on challenging benchmarks like AIME and GPQA Diamond. This demonstrates an unexpected <strong>generalization benefit</strong> for advanced reasoning models.</p>
                        <p lang="zh">将<strong>Enigmata-Data</strong>合成谜题数据集纳入大规模模型训练，例如<a href="https://arxiv.org/abs/2504.13914">Seed1.5-Thinking</a>，令人惊讶地提高了在AIME和GPQA Diamond等具有挑战性的基准测试上的性能。这展示了高级推理模型的意外<strong>泛化优势</strong>。</p>
                    </div>

                      <figure>
                        <img src="./assets/seed_thinking.png" alt="A descriptive alt text for your image" style="width: 600px;">
                        <figcaption style="color: gray;">Results on benchmarks for general reasoning capabilities</figcaption>
                      </figure>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <!-- <section class="section">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-10">
                    <h2 class="title is-3">Performance Results</h2>
                    <div class="content">
                        
                       
                            
                        <div class="content">
                            <p>Our model, specifically the 32B parameter version, significantly outperforms most public models on Enigmata-Eval and ARC-AGI, showcasing enhanced general logical reasoning. This success stems from effective rejection fine-tuning (RFT) and multi-task RL strategies, which improve generalization while preserving existing math reasoning abilities.</p>
                        </div>
                        <figure>
                            <img src="./assets/main_results_1.png" alt="A descriptive alt text for your image" style="width: 600px;">
                            <figcaption style="color: gray;">Performance of reasoning, generic, and our trained LLMs on reasoning benchmarks</figcaption>
                          </figure>
                        
                          <div class="content">
                            <p>On Enigmata-Eval, our Qwen2.5-32B-Enigmata model excels in Crypto, Arithmetic, and Logic tasks, indicating strong rule-based reasoning. It also performs competitively in search tasks, though spatial and sequential categories remain challenging.</p>
                        </div>

                          <figure>
                            <img src="./assets/main_results2.png" alt="A descriptive alt text for your image" style="width: 600px;">
                            <figcaption style="color: gray;">Performance of reasoning LLMs, generic LLMs, and our trained LLMs on Enigmata-Eval</figcaption>
                          </figure>

                          <div class="content">
                            <p>Incorporating the <strong>Enigmata-Data</strong> synthetic puzzle dataset into large-scale model training significantly boosts general reasoning, including math and STEM, surprisingly improving performance on challenging benchmarks like AIME and GPQA Diamond. This demonstrates an unexpected <strong>generalization benefit</strong> for advanced reasoning models.</p>
                        </div>

                          <figure>
                            <img src="./assets/seed_thinking.png" alt="A descriptive alt text for your image" style="width: 600px;">
                            <figcaption style="color: gray;">
                                <span lang="en">Results on benchmarks for general reasoning capabilities</span>
                                <span lang="zh">通用推理能力基准测试的结果</span>
                            </figcaption>
                          </figure>


                    </div>
                </div>
            </div>
        </div>
    </section> -->

    <section class="section" id="citation">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-10">
                    <h2 class="title is-3">
                        <span lang="en">📝 Citation</span>
                        <span lang="zh">📝 引用</span>
                    </h2>
                    <div class="content">
                        <p lang="en">If you find this work useful, please cite our paper:</p>
                        <p lang="zh">如果您发现这项工作有用，请引用我们的论文：</p>
                        <pre><code class="latex">
@article{chen2025enigmata,
    title={Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles},
    author={Chen, Jiangjie and He, Qianyu and Yuan, Siyu and Chen, Aili and Cai, Zhicheng and Dai, Weinan and Yu, Hongli and Yu, Qiying and Li, Xuefeng and Chen, Jiaze and others},
    journal={arXiv preprint arXiv:2505.19914},
    year={2025}
}
                        </code></pre>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <div class="has-text-centered">
                <p>
                    <span lang="en">© 2025 <a href="https://seed.bytedance.com">ByteDance Seed</a>. Modified from <a href="https://github.com/seed-enigmata/seed-enigmata.github.io">R2E-Gym</a>.</span>
                    <span lang="zh">© 2025 <a href="https://seed.bytedance.com">字节跳动Seed</a>. 修改自 <a href="https://github.com/seed-enigmata/seed-enigmata.github.io">R2E-Gym</a>.</span>
                </p>
            </div>
        </div>
    </footer>


</body>


</html>
