<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
  <title>RL-Memory Agent: 长文本外推新范式</title>
  <style>
    body {
      font-family: "Helvetica Neue", sans-serif;
      line-height: 1.6;
      max-width: 900px;
      margin: 2em auto;
      padding: 0 1em;
      color: #333;
    }
    h1, h2, h3 {
      color: #2c3e50;
    }
    a {
      color: #2980b9;
      text-decoration: none;
    }
    .links {
      background: #f9f9f9;
      border-left: 5px solid #2980b9;
      padding: 1em;
      margin-bottom: 2em;
    }
    img {
      max-width: 100%;
      margin: 1em 0;
    }
  </style>
</head>
<body>

<h1>Refreshing Long-Context for LLMs:<br> The Memory Agent Paradigm via Reinforcement Learning</h1>

<div class="links">
  <strong>📄 论文地址：</strong> <a href="https://arxiv.org/abs/xxxx.xxxxx" target="_blank">ArXiv PDF</a><br>
  <strong>🧠 模型权重：</strong> <a href="https://huggingface.co/your-org/RL-Memory-Agent" target="_blank">HuggingFace</a><br>
  <strong>💻 代码仓库：</strong> <a href="https://github.com/your-org/RL-Memory-Agent" target="_blank">GitHub Repo</a>
</div>

<h2>导读：</h2>
<p>我们提出了一种全新的长文本处理框架——<strong>RL-Memory Agent</strong>，通过强化学习优化大语言模型处理长文档的能力，无需修改模型架构，突破了Transformer上下文窗口限制、计算复杂度瓶颈与外推性能退化三重挑战。</p>

<h2>三大核心突破：</h2>
<ul>
  <li><strong>记忆机制创新：</strong> 模拟人类笔记本式记忆，将长文本分段处理，并通过可训练的写入策略动态更新记忆。</li>
  <li><strong>线性复杂度：</strong> 计算和显存需求随文本长度线性增长，打破传统Transformer的O(n²)瓶颈。</li>
  <li><strong>强化学习驱动的外推：</strong> 改进GRPO算法支持多轮训练，成功实现从32K训练到3.5M上下文的无损外推。</li>
</ul>

<h2>方法概览：</h2>
<h3>Memory Agent 架构</h3>
<p>模型引入了辅助记忆面板，将输入拆分为固定长度的若干块，并以循环方式逐块读取与写入记忆，实现可控的长距离依赖建模。</p>
<img src="memory-agent-diagram.png" alt="Memory Agent架构图">

<h3>建模公式</h3>
<p>引入潜在记忆变量后，长文本生成的概率可重写为嵌套结构，包含<strong>读</strong>与<strong>写</strong>两个阶段，极大增强了序列建模能力。</p>

<h2>RL训练机制：</h2>
<ul>
  <li><strong>对话并行训练：</strong> 每个样本生成多个独立对话轨迹。</li>
  <li><strong>奖励归一分配：</strong> 根据最终答案评分，回传至每轮对话。</li>
  <li><strong>分块优化：</strong> 三维损失结构（组别、轮次、token）支持稳定训练。</li>
</ul>
<img src="rl-training-flow.png" alt="RL训练流程图">

<h2>实验结果</h2>
<h3>主实验结果</h3>
<ul>
  <li>QwenLong、Qwen-1M、DS系列模型在超长文本处理能力上均出现性能崩溃。</li>
  <li>RL-Memory Agent-14B模型在3.5M Token任务中几乎无性能衰减。</li>
</ul>
<img src="main-results.png" alt="主实验结果图">

<h3>消融实验</h3>
<p>强化学习显著提升记忆模块外推能力。</p>
<img src="ablation.png" alt="消融实验图">

<h3>RULER基准测试</h3>
<ul>
  <li><strong>任务类型：</strong> Needle-in-Haystack, Variable Tracking, Aggregation, QA</li>
  <li><strong>RL-Memory Agent：</strong> 在各类任务中全面领先，尤其在多问题任务中稳定性远超同类模型。</li>
</ul>
<img src="ood-heatmap.png" alt="RULER热力图">

<h2>系统架构创新（彩蛋）</h2>
<ul>
  <li><strong>异步执行：</strong> CPU/GPU分离，资源利用最大化</li>
  <li><strong>接口统一：</strong> 完整兼容OpenAI API与VLLM，支持任意多Agent工作流</li>
  <li><strong>任务调度优化：</strong> 自动回收与分配计算资源，实现高并发多轮对话训练</li>
</ul>
<img src="system-async-arch.png" alt="系统架构图">

<hr>
<p style="text-align:center; font-size: 0.9em; color: #777;">© 2025 RL-Memory Agent Team</p>

</body>
</html>
