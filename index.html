<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="description"
        content="Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles" />
    <meta name="keywords, puzzle, logical, reasoning, large lanuge model, LLM"
        content="Enigmata" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>MemAgent: Reshaping Long-Context LLM with Mult-Conv RL based Memory Agent</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />
    <link rel="stylesheet" href="./css/bulma.min.css" />
    <link rel="stylesheet" href="./css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="./css/bulma-slider.min.css" />
    <link rel="stylesheet" href="./css/fontawesome.all.min.css" />
    <!-- <link rel="stylesheet" href="./css/fontawesome_6_7_2.all.css" /> -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
    <link rel="stylesheet" href="./css/index.css" />
    <link rel="stylesheet" href="./css/enhanced-styles.css" />
    <link rel="stylesheet" href="./css/final-enhancements.css" />
    <link rel="stylesheet" href="./css/navbar-fix.css" />
    <link rel="stylesheet" href="./css/navbar-alignment-fix.css" />
    <link rel="stylesheet" href="./css/overlap-fix.css" />
    <link rel="stylesheet" href="./css/spacing-fix.css" />
    <link rel="stylesheet" href="./css/author-affiliation-styles.css" />
    <link rel="icon" href="./assets/doubao.png" type="image/png" />
    <!-- add page icon at 64 x 64-->
    <!-- <link rel="icon" type="image/png" href="./assets/re.png" sizes="256x256" /> -->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./js/fontawesome.all.min.js"></script>
    <script src="./js/bulma-carousel.min.js"></script>
    <script src="./js/bulma-slider.min.js"></script>
    <script src="./js/index.js"></script>
    <script src="./js/enhanced-animations.js"></script>
    <script src="./js/language-switcher.js"></script>
    <script src="https://cdn.tailwindcss.com"></script>
</head>

<body>
    <!-- Enhanced Navigation Bar -->
    <nav class="navbar is-fixed-top" role="navigation" aria-label="main navigation">
        <div class="container">
            <div class="navbar-brand">
                <a class="navbar-item" href="#">
                    🧩 MemAgent
                </a>
                <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                </a>
            </div>
            <div class="navbar-menu">
                <div class="navbar-end">
                    <a class="navbar-item" href="#introduction">
                        <span lang="en">Introduction</span>
                        <span lang="zh">介绍</span>
                    </a>
                    <a class="navbar-item" href="#data">
                        <span lang="en">Data</span>
                        <span lang="zh">数据</span>
                    </a>
                    <a class="navbar-item" href="#eval">
                        <span lang="en">Evaluation</span>
                        <span lang="zh">评估</span>
                    </a>
                    <a class="navbar-item" href="#model">
                        <span lang="en">Model</span>
                        <span lang="zh">模型</span>
                    </a>
                    <a class="navbar-item" href="#citation">
                        <span lang="en">Citation</span>
                        <span lang="zh">引用</span>
                    </a>
                    <button id="language-toggle" class="navbar-item language-toggle">中文</button>
                </div>
            </div>
        </div>
    </nav>
    
    <section class="hero">
        <div class="hero-body">
            <div class="container">
                <div class="has-text-centered">
                    <h1 class="publication-title">
                        <span lang="en"><em class="dnerf">Enigmata</em>: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles</span>
                        <span lang="zh"><em class="dnerf">Enigmata</em>: 通过合成可验证谜题扩展大语言模型的逻辑推理能力</span>
                    </h1>
                    <div class="publication-authors">
                        <span class="author-block">
                            <span lang="en"><a href="https://seed-enigmata.github.io/">Hongli Yu</a><sup>1,3,6</sup></span>
                            <span lang="zh"><a href="https://seed-enigmata.github.io/">于鸿利</a><sup>1,3,6</sup></span>
                        </span>
                        <span class="author-block">
                            <span lang="en"><a href="https://zhouh.github.io/">Hao Zhou</a><sup>3,6</sup></span>
                            <span lang="zh"><a href="https://zhouh.github.io/">周浩</a><sup>3,6</sup></span>
                        </span>
                    </div>
                    <div class="publication-affiliations">
                        <span class="affiliation-block">
                            <span lang="en"><sup>1</sup>ByteDance Seed</span>
                            <span lang="zh"><sup>1</sup>字节跳动 Seed</span>
                        </span><br>
                        <span class="affiliation-block">
                            <span lang="en"><sup>3</sup>Institute for AI Industry Research (AIR), Tsinghua University</span>
                            <span lang="zh"><sup>3</sup>清华大学 智能产业研究院（AIR）</span>
                        </span><br>
                        <span class="affiliation-block">
                            <span lang="en"><sup>6</sup>SIA-Lab of Tsinghua AIR and ByteDance Seed</span>
                            <span lang="zh"><sup>6</sup>清华-字节联合实验室SIA-Lab</span>
                        </span>
                        <div class="affiliation-note">
                            <span lang="en">*Project Lead; †Equal Contribution</span>
                            <span lang="zh">*项目负责人; †同等贡献</span>
                        </div>
                        <div class="affiliation-note">
                            <span lang="en">Contact: <a href="mailto:jiangjiec@bytedance.com">jiangjiec@bytedance.com</a></span>
                            <span lang="zh">联系方式: <a href="mailto:jiangjiec@bytedance.com">jiangjiec@bytedance.com</a></span>
                        </div>
                    </div>
                    <div class="publication-links">
                        <span class="link-block">
                            <a href="http://arxiv.org/abs/2505.19914" class="external-link button is-dark">
                                <span class="icon">
                                    <i class="fas fa-file-pdf"></i>
                                </span>
                                <span lang="en">Paper</span>
                                <span lang="zh">论文</span>
                            </a>
                        </span>
                        <span class="link-block">
                            <a href="https://github.com/BytedTsinghua-SIA/Enigmata" class="external-link button is-dark">
                                <span class="icon">
                                    <i class="fas fa-database"></i>
                                </span>
                                <span lang="en">Code</span>
                                <span lang="zh">代码</span>
                            </a>
                        </span>
                        <span class="link-block">
                            <a href="https://huggingface.co/datasets/BytedTsinghua-SIA/Enigmata-Eval" class="external-link button is-dark">
                                <span class="icon">
                                    <i class="fas fa-database"></i>
                                </span>
                                <span lang="en">Evaluation</span>
                                <span lang="zh">评估</span>
                            </a>
                        </span>
                        <span class="link-block">
                            <a href="https://huggingface.co/BytedTsinghua-SIA/Enigmata-Qwen2.5-32B" class="external-link button is-dark">
                                <span class="icon">
                                    🤗
                                </span>
                                <span lang="en">Model</span>
                                <span lang="zh">模型</span>
                            </a>
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </section>

<section class="section" id="introduction">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-10">
                <h2 class="title is-3">
                    <span lang="en">Introduction</span>
                    <span lang="zh">引言</span>
                </h2>
                <div class="content">

                    <p lang="en">
                        We present <strong>RL-Memory Agent</strong>, a novel framework for long-text processing that directly optimizes task performance through end-to-end reinforcement learning, without any architectural modifications to the base model.
                    </p>
                    <p lang="zh">
                        我们推出了<strong>RL-Memory Agent</strong>，这是一个全新的长文本处理框架，能够通过端到端的强化学习直接优化长文本任务性能，而无需更改底层模型架构。
                    </p>
                    <p lang="en">
                        <strong>RL-Memory Agent</strong> achieves three major breakthroughs:
                    </p>

                    <p lang="zh">
                        <strong>RL-Memory Agent</strong> 实现了三大核心突破：
                    </p>
                    <div
                        style="background-color: #f8fafc; border-radius: 12px; border-left: 6px solid #3a76ed; padding: 1.2rem; margin-bottom: 1.5rem;">
                            <div style="display: flex; flex-direction: column; gap: 0.8rem;">
                                <div style="display: flex; align-items: flex-start; gap: 0.8rem;"> 
                                    <div
                                        style="background-color: #3a76ed; color: white; padding: 0.4rem; border-radius: 50%; font-size: 0.9rem; display: flex; align-items: center; justify-content: center; min-width: 2rem; height: 2rem;">
                                        1
                                    </div>
                                    <div>
                                        <p style="margin: 0; line-height: 1.4;" lang="en"><strong>Novel memory mechanism: </strong>
                                            It introduces a notebook-style workflow where the agent reads input in segments and efficiently updates memory using an overwrite strategy. This enables handling virtually unlimited-length inputs within a fixed context window, fundamentally breaking the hard length limits of Transformer models.</p>
                                        <p style="margin: 0; line-height: 1.4;" lang="zh"><strong>新型记忆机制：</strong>
                                            我们引入了一种记事本式的工作流，Agent 以分段方式读取文本，并借助覆写策略高效更新记忆。该设计使得模型能够在固定上下文窗口内处理几乎任意长度的输入，从根本上突破了传统 Transformer 架构的窗口长度限制。</p>
                                    </div>
                                </div>
                        
                                <div style="display: flex; align-items: flex-start; gap: 0.8rem;">
                                    <div
                                        style="background-color: #3a76ed; color: white; padding: 0.4rem; border-radius: 50%; font-size: 0.9rem; display: flex; align-items: center; justify-content: center; min-width: 2rem; height: 2rem;">
                                        2
                                    </div>
                                    <div>
                                    <p style="margin: 0; line-height: 1.4;" lang="en"><strong>O(n) linear complexity:</strong> 
                                        The agent design decouples compute and memory cost from input length, reducing the processing burden from quadratic to linear with respect to input size.</p>
                                    <p style="margin: 0; line-height: 1.4;" lang="zh"><strong>O(n) 线性复杂度：</strong>
                                        这一创新性的 Agent 设计将计算与显存开销与文本长度解耦，使得处理长文本的复杂度由原本的二次方增长转变为线性增长。</p>
                                    </div>
                                </div>

                                <div style="display: flex; align-items: flex-start; gap: 0.8rem;">
                                    <div
                                        style="background-color: #3a76ed; color: white; padding: 0.4rem; border-radius: 50%; font-size: 0.9rem; display: flex; align-items: center; justify-content: center; min-width: 2rem; height: 2rem;">
                                        3
                                    </div>
                                    <div>
                                        <p style="margin: 0; line-height: 1.4;" lang="en"><strong>RL-driven extrapolation:</strong>
                                            A modified GRPO algorithm supports training on multi-turn generation tasks with independent contexts. As a result, models trained with 32K contexts can extrapolate to 3.5M-token sequences with <5% performance drop.</p>
                                        <p style="margin: 0; line-height: 1.4;" lang="zh"><strong>强化学习驱动的外推能力:</strong>
                                            我们改进了 GRPO 算法，使其支持独立上下文的多轮生成训练。基于此，训练出的 Memory Agent 在32K上下文长度的数据上训练后，能够无损外推至3.5M token的问答任务中，性能下降低于5%。</p>
                                    </div>
                                </div>                                
                        </div>
                    </div>

                    <p lang="en">
                        In a simple yet effective way, we demonstrate—for the first time—a trainable memory ability empowered by reinforcement learning. RL-Memory Agent showcases the untapped potential of optimizing specialized workflows via policy learning rather than architectural changes.
                    </p>
                    <p lang="zh">
                        我们以一种简洁而高效的方式，首次实现了真正意义上的、由强化学习赋予的可训练记忆能力，充分展现了强化学习在优化特定工作流方面的巨大潜力。
                    </p>

                    <strong><em style="color: #3a76ed"><center lang="en">We believe RL-Memory Agent paves the way for the next generation of scalable long-context LLMs.</center></em></strong>
                    <strong><em style="color: #3a76ed"><center lang="zh">我们相信，RL-Memory Agent 为下一代可扩展的长上下文大语言模型开辟了新路径。</center></em></strong>

                </div>
            </div>
        </div>
    </div>
</section>

<section class="section" id="method">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-10">
                <h2 class="title is-3">
                    <span lang="en">Method</span>
                    <span lang="zh">方法介绍</span>
                </h2>
                <div class="content">

                    <p lang="en">
                        Inspired by human strategies for long-text comprehension and neural memory models such as Neural Turing Machines and Memory Networks, we propose a novel architecture-free framework called <strong>Memory Agent</strong>, which equips LLMs with a dynamically updated memory module.
                    </p>
                    <p lang="zh">
                        受人类处理长文本时的行为模式启发，并借鉴了Neural Turing Machine和Memory Network等神经网络与记忆机制结合的工作，我们提出了一种无需修改模型架构的长文处理新方案——<strong>Memory Agent</strong>，
                        <br><strong style="color:#3273dc;">给LLM装上动态更新的记忆模块</strong>。
                    </p>

                    <p lang="en">
                        <strong>Memory Agent</strong> introduces a fixed-size auxiliary memory panel that allows the model to read long texts in segments and update memory after each segment. This forms a new "local processing + global fusion" workflow. During inference, the memory is dynamically written and aggregated to generate the final output.
                    </p>
                    <p lang="zh">
                        <strong>Memory Agent</strong>引入了一个固定长度的辅助记忆面板，允许模型在处理长文本时以分段方式读取输入，并在每一段后主动更新记忆状态，从而实现“局部处理 + 全局融合”的新型工作流。该记忆模块在推理过程中不断更新，并在所有段落处理完毕后聚合关键信息生成最终输出。
                    </p>

                    <figure>
                        <img src="figs/method_00.png" alt="Memory Agent Architecture Overview">
                    </figure>

                    <p lang="en">
                        <strong>Traditional Modeling:</strong><br>
                        $$p(\mathbf{x}_{1:N})=p(x_1)\prod_{n=2}^N p(x_n \mid \mathbf{x}_{1:n-1})$$
                        This requires caching all previous tokens, leading to quadratic complexity.
                    </p>
                    <p lang="zh">
                        <strong>传统建模方式：</strong><br>
                        $$p(\mathbf{x}_{1:N})=p(x_1)\prod_{n=2}^N p(x_n \mid \mathbf{x}_{1:n-1})$$
                        这种方式需要缓存全部历史token，导致计算与显存开销呈二次方增长。
                    </p>

                    <p lang="en">
                        <strong>Memory Agent Process:</strong><br>
                        $$p(\mathbf{x}_{1:N}) = \sum_{\mathbf{m}^{1:K-1}} \prod_{k=1}^{K}
                        \underbrace{p(\mathbf{c}^k \mid \mathbf{m}^{k-1})}_{\text{read}} \cdot
                        \underbrace{p(\mathbf{m}^k \mid \mathbf{c}^k, \mathbf{m}^{k-1})}_{\text{write}}$$
                    </p>
                    <p lang="zh">
                        <strong>Memory Agent 核心流程：</strong><br>
                        $$p(\mathbf{x}_{1:N}) = \sum_{\mathbf{m}^{1:K-1}} \prod_{k=1}^{K}
                        \underbrace{p(\mathbf{c}^k \mid \mathbf{m}^{k-1})}_{\text{读取}} \cdot
                        \underbrace{p(\mathbf{m}^k \mid \mathbf{c}^k, \mathbf{m}^{k-1})}_{\text{写入}}$$
                    </p>

                    <h4 lang="en"><strong>Reinforcement Learning Optimization</strong></h4>
                    <h4 lang="zh"><strong>强化学习训练机制</strong></h4>

                    <p lang="en">

                    </p>
                    <p lang="zh">
                        我们使用目前在推理领域表现出卓越性能的基于可验证结果的强化学习（RLVR)来训练Memory agent，而非简单的进行微调或指令工程。为此，我们扩展了现有的DAPO算法，使其进一步支持了具有多轮独立对话的AgentWorkflow的端到端优化。
                    </p>

                    <div class="columns">
                        <div class="column">
                            <figure>
                                <img src="figs/algo_00.png" alt="RL Flow">
                            </figure>
                        </div>
                        <div class="column">
                            <figure>
                                <img src="figs/template.png" alt="RL Template">
                            </figure>
                        </div>
                    </div>

                    <h4 lang="en"><strong>Multi-Turn Dialog RL Training</strong></h4>
                    <h4 lang="zh"><strong>多轮对话强化学习机制</strong></h4>

                    <p lang="en">
                        For each sample, multiple independent dialog sequences are generated in parallel. The final answer is selected to compute a reward, which is then distributed intelligently across all related dialogues. Chunk-level token losses are computed under a 3D optimization structure: (group, turn, token).
                    </p>
                    <p lang="zh">
                        在训练过程中，每个输入样本会并行生成多组独立的对话序列。系统根据最终的回答确定奖励值，并通过组归一化方式将其智能分配到所有相关对话中。损失以chunk为单位计算，优化结构包含“组别-对话轮次-token”三维。
                    </p>

                    <p>
                        $$ 
                        \begin{aligned}
                        \mathcal{J}_{\text{GRPO}}(\theta) =\quad &\mathbb{E}_{(q,a)\sim \mathcal{D}, \{o_{i,j}\}_{i=1}^G\sim \pi_{\theta_\text{old}}(\cdot\mid q,~o_{i,j-1})} \\
                        &\Bigg[\frac{1}{\sum_{i=1}^{G}\sum_{j=1}^{n_i}|o_{i,j}|}\sum_{i=1}^{G}\sum_{j=1}^{n_i}\sum_{t=1}^{|o_{i,j}|}
                        \Big(\mathcal{C}_{i,j,t} - \beta D_{\text{KL}}(\pi_{\theta} || \pi_{\text{ref}} )\Big) \Bigg] \\
                        \text{where\quad} \mathcal{C}_{i,j,t} = &\min\Big(r_{i,j,t}(\theta) \hat{A}_{i,j,t},  
                        \ \text{clip} \Big( r_{i,j,t}(\theta), 1 - {\varepsilon}, 1 + {\varepsilon} \Big) \hat{A}_{i,j,t}\Big)
                        \end{aligned}
                        $$
                    </p>

                </div>
            </div>
        </div>
    </div>
</section>


<section class="section" id="experiments">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-10">
        <h2 class="title is-3">
          <span lang="en">Experiments</span>
          <span lang="zh">实验分析</span>
        </h2>
        <div class="content">

          <h4 lang="en"><strong>Main Results</strong></h4>
          <h4 lang="zh"><strong>主实验结果</strong></h4>

          <p lang="zh">
            <strong>基线模型：</strong> 实验显示，现有长文本模型在超长序列上普遍存在严重的性能退化：
            <ul>
              <li>QwenLong-L1系列：在文本长度超过56K后准确率急剧下降，降幅超过40个百分点。</li>
              <li>Qwen2.5-Instruct-1M系列：标称支持1M，但在896K时性能降为0。</li>
              <li>DS-distill系列：性能随长度呈指数级衰减，在224K时几乎完全失效。</li>
            </ul>
          </p>

          <p lang="zh">
            <strong>RL-Memory Agent：</strong> 在超长文本处理任务中表现出极高稳定性：
            <ul>
              <li>RL-Memory Agent-14B：在3.5M超长文本测试中仅出现<5%的边际性能下降，实现了无损外推。</li>
              <li>RL-Memory Agent-7B：在最长文本上虽略有性能下降，但总体表现远超其他模型。</li>
            </ul>
          </p>

          <figure>
            <img src="figs/main_result_00.png" alt="Main Results">
          </figure>

          <h4 lang="zh"><strong>消融实验</strong></h4>
          <p lang="zh">
            为验证Memory Agent各组件的贡献，我们开展了系统的消融实验：
            <ol>
              <li><strong>基础模型：</strong> 在超出训练窗口后，信息缺失导致性能急剧下降，无法实现外推。</li>
              <li><strong>Memory Agent（未RL训练）：</strong> 具备一定处理能力，但在长文本任务下仍出现明显性能损失。</li>
              <li><strong>RL-Memory Agent：</strong> 强化学习训练显著提升外推能力，达到近乎无损性能。</li>
            </ol>
          </p>

          <figure>
            <img src="figs/ablation_00.png" alt="Ablation Study">
          </figure>

          <h4 lang="zh"><strong>RULER基准测试与OOD能力分析</strong></h4>
          <p lang="zh">
            <strong>RULER基准测试：</strong> 这是当前长文本外推能力研究的标准测试集，其核心优势是可控长度生成任务，主要包含以下四大任务类型：
            <ul>
              <li><strong>大海捞针（Needle-in-a-Haystack）：</strong> 在超长文本中定位关键needle，包含8类干扰变体。</li>
              <li><strong>变量追踪（Variable Tracking）：</strong> 模拟程序分析场景，追踪变量引用和赋值关系。</li>
              <li><strong>聚合任务（Aggregation）：</strong> 汇总分散信息，评估模型对全局特征的掌握能力。</li>
              <li><strong>问答任务（QA）：</strong> 进行多跳复杂推理，测试模型上下文理解与问答能力。</li>
            </ul>
          </p>

          <p lang="zh">
            <strong>实验结果分析：</strong> 我们使用热力图可视化不同模型在不同长度区间和任务类型下的性能：
            <ul>
              <li><strong>RL-Memory Agent：</strong> 所有任务类型上均优于现有模型，展现显著性能优势。</li>
              <li><strong>Memory Agent（未训练）：</strong> 在大多数情况下优于基础模型，但外推长度上仍出现性能下降。</li>
              <li><strong>基础模型：</strong> Distill/Instruct系列在超长文本任务中普遍失败。</li>
              <li><strong>QwenLong系列：</strong> 在超长文本下性能剧降，甚至不如基础模型。</li>
              <li><strong>Qwen-1M系列：</strong> 在简单任务上表现尚可，但在复杂推理任务中急剧下降。</li>
            </ul>
          </p>

          <div class="columns is-multiline">
            <div class="column is-4">
              <figure><img src="figs/niah_single_1_00.png" alt="NIAH Single 1"></figure>
            </div>
            <div class="column is-4">
              <figure><img src="figs/niah_single_2_00.png" alt="NIAH Single 2"></figure>
            </div>
            <div class="column is-4">
              <figure><img src="figs/niah_single_3_00.png" alt="NIAH Single 3"></figure>
            </div>

            <div class="column is-4">
              <figure><img src="figs/niah_multikey_1_00.png" alt="NIAH Multi 1"></figure>
            </div>
            <div class="column is-4">
              <figure><img src="figs/niah_multikey_2_00.png" alt="NIAH Multi 2"></figure>
            </div>
            <div class="column is-4">
              <figure><img src="figs/niah_multikey_3_00.png" alt="NIAH Multi 3"></figure>
            </div>

            <div class="column is-6">
              <figure><img src="figs/niah_multiquery_00.png" alt="Multi-query"></figure>
            </div>
            <div class="column is-6">
              <figure><img src="figs/niah_multivalue_00.png" alt="Multi-value"></figure>
            </div>

            <div class="column is-6">
              <figure><img src="figs/fwe_00.png" alt="FWE"></figure>
            </div>
            <div class="column is-6">
              <figure><img src="figs/vt_00.png" alt="VT"></figure>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="conclusion">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-10">
        <h2 class="title is-3">
          <span lang="en">Conclusion</span>
          <span lang="zh">总结</span>
        </h2>
        <div class="content">
          <p lang="zh">
            本研究提出了一种全新的长文本处理框架 <strong>RL-Memory Agent</strong>，在以下三个关键维度实现了重要突破：
          </p>
          <ul>
            <li><strong>技术架构突破：</strong> 提出一种创新机制，使大语言模型在固定窗口限制下仍能以 <strong>线性复杂度</strong>处理任意长度文本，从根本上突破传统Transformer的计算瓶颈。</li>
            <li><strong>智能体训练方法：</strong> 构建了一整套强化学习驱动的智能体工作流，基于 <strong>多对话GRPO</strong> 实现端到端训练机制。</li>
            <li><strong>性能外推验证：</strong> 大量实验表明，RL训练能够帮助模型外推至远超训练窗口的任务中，<strong>性能下降<5%</strong>，显著扩展LLM能力边界。</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="engineering">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-10">
        <h2 class="title is-3">
          <span lang="en">Engineering Bonus</span>
          <span lang="zh">工程彩蛋</span>
        </h2>
        <div class="content">
          <p lang="zh">
            <strong>RL-Memory Agent</strong> 的成功不仅依赖于理论方法，还得益于工程系统的创新性支持。我们提出了一种适配大模型训练的新型异步架构，并实现了接口统一，显著提升了训练效率与易用性。
          </p>
          <ul>
            <li><strong>纯异步流水线：</strong> GPU/CPU资源解耦，<code>AsyncLLMEngine</code> 负责多节点推理，<code>Ray Worker</code>管理常驻CPU任务池，通过协程完成资源调度。</li>
            <li><strong>统一API接口：</strong> 兼容 OpenAI API 与 vLLM 推理，支持 <strong>多轮工具调用、多Agent并行、多任务训练</strong>，真正消除传统状态机地狱。</li>
            <li><strong>易用性显著提升：</strong> 训练流程只需编写一个标准 OpenAI 接口函数，即可完成复杂上下文拼接、token处理与状态管理。</li>
          </ul>

          <figure>
            <img src="figs/workflow1.png" alt="Async Engine Workflow 1">
          </figure>
          <figure>
            <img src="figs/workflow2.png" alt="Async Engine Workflow 2">
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>


    <section class="section" id="citation">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-10">
                    <h2 class="title is-3">
                        <span lang="en">📝 Citation</span>
                        <span lang="zh">📝 引用</span>
                    </h2>
                    <div class="content">
                        <p lang="en">If you find this work useful, please cite our paper:</p>
                        <p lang="zh">如果您发现这项工作有用，请引用我们的论文：</p>
                        <pre><code class="latex">
@article{chen2025enigmata,
    title={Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles},
    author={Chen, Jiangjie and He, Qianyu and Yuan, Siyu and Chen, Aili and Cai, Zhicheng and Dai, Weinan and Yu, Hongli and Yu, Qiying and Li, Xuefeng and Chen, Jiaze and others},
    journal={arXiv preprint arXiv:2505.19914},
    year={2025}
}
                        </code></pre>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <div class="has-text-centered">
                <p>
                    <span lang="en">© 2025 <a href="https://seed.bytedance.com">ByteDance Seed</a>. Modified from <a href="https://github.com/seed-enigmata/seed-enigmata.github.io">R2E-Gym</a>.</span>
                    <span lang="zh">© 2025 <a href="https://seed.bytedance.com">字节跳动Seed</a>. 修改自 <a href="https://github.com/seed-enigmata/seed-enigmata.github.io">R2E-Gym</a>.</span>
                </p>
            </div>
        </div>
    </footer>


</body>


</html>
