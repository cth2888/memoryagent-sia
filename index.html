<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="description"
        content="Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles" />
    <meta name="keywords, puzzle, logical, reasoning, large lanuge model, LLM"
        content="Enigmata" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />
    <link rel="stylesheet" href="./css/bulma.min.css" />
    <link rel="stylesheet" href="./css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="./css/bulma-slider.min.css" />
    <link rel="stylesheet" href="./css/fontawesome.all.min.css" />
    <!-- <link rel="stylesheet" href="./css/fontawesome_6_7_2.all.css" /> -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
    <link rel="stylesheet" href="./css/index.css" />
    <link rel="stylesheet" href="./css/enhanced-styles.css" />
    <link rel="stylesheet" href="./css/final-enhancements.css" />
    <link rel="stylesheet" href="./css/navbar-fix.css" />
    <link rel="stylesheet" href="./css/navbar-alignment-fix.css" />
    <link rel="stylesheet" href="./css/overlap-fix.css" />
    <link rel="stylesheet" href="./css/spacing-fix.css" />
    <link rel="stylesheet" href="./css/author-affiliation-styles.css" />
    <link rel="icon" href="./assets/doubao.png" type="image/png" />
    <!-- add page icon at 64 x 64-->
    <!-- <link rel="icon" type="image/png" href="./assets/re.png" sizes="256x256" /> -->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./js/fontawesome.all.min.js"></script>
    <script src="./js/bulma-carousel.min.js"></script>
    <script src="./js/bulma-slider.min.js"></script>
    <script src="./js/index.js"></script>
    <script src="./js/enhanced-animations.js"></script>
    <script src="./js/language-switcher.js"></script>
    <script src="https://cdn.tailwindcss.com"></script>
</head>

<body>
    <!-- Enhanced Navigation Bar -->
    <nav class="navbar is-fixed-top" role="navigation" aria-label="main navigation">
        <div class="container">
            <div class="navbar-brand">
                <a class="navbar-item" href="#">
                    ğŸ§© Enigmata
                </a>
                <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                </a>
            </div>
            <div class="navbar-menu">
                <div class="navbar-end">
                    <a class="navbar-item" href="#introduction">
                        <span lang="en">Introduction</span>
                        <span lang="zh">ä»‹ç»</span>
                    </a>
                    <a class="navbar-item" href="#data">
                        <span lang="en">Data</span>
                        <span lang="zh">æ•°æ®</span>
                    </a>
                    <a class="navbar-item" href="#eval">
                        <span lang="en">Evaluation</span>
                        <span lang="zh">è¯„ä¼°</span>
                    </a>
                    <a class="navbar-item" href="#model">
                        <span lang="en">Model</span>
                        <span lang="zh">æ¨¡å‹</span>
                    </a>
                    <a class="navbar-item" href="#citation">
                        <span lang="en">Citation</span>
                        <span lang="zh">å¼•ç”¨</span>
                    </a>
                    <button id="language-toggle" class="navbar-item language-toggle">ä¸­æ–‡</button>
                </div>
            </div>
        </div>
    </nav>
    
    <section class="hero">
        <div class="hero-body">
            <div class="container">
                <div class="has-text-centered">
                    <h1 class="publication-title">
                        <span lang="en"><em class="dnerf">Enigmata</em>: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles</span>
                        <span lang="zh"><em class="dnerf">Enigmata</em>: é€šè¿‡åˆæˆå¯éªŒè¯è°œé¢˜æ‰©å±•å¤§è¯­è¨€æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›</span>
                    </h1>
                    <div class="publication-authors">
                        <span class="author-block">
                            <span lang="en"><a href="https://seed-enigmata.github.io/">Hongli Yu</a><sup>1,3,6</sup></span>
                            <span lang="zh"><a href="https://seed-enigmata.github.io/">äºé¸¿åˆ©</a><sup>1,3,6</sup></span>
                        </span>
                        <span class="author-block">
                            <span lang="en"><a href="https://scholar.google.com/citations?user=DDRBbxgAAAAJ&hl=zh-CN">Xuefeng Li</a><sup>1,5</sup></span>
                            <span lang="zh"><a href="https://scholar.google.com/citations?user=DDRBbxgAAAAJ&hl=zh-CN">æå­¦å³°</a><sup>1,5</sup></span>
                        </span>
                        <span class="author-block">
                            <span lang="en"><a href="https://scholar.google.com/citations?user=Vt1j3kEAAAAJ&hl=zh-CN">Jiaze Chen</a><sup>1</sup></span>
                            <span lang="zh"><a href="https://scholar.google.com/citations?user=Vt1j3kEAAAAJ&hl=zh-CN">é™ˆå®¶æ³½</a><sup>1</sup></span>
                        </span>
                        <span class="author-block">
                            <span lang="en"><a href="https://zhouh.github.io/">Hao Zhou</a><sup>3,6</sup></span>
                            <span lang="zh"><a href="https://zhouh.github.io/">å‘¨æµ©</a><sup>3,6</sup></span>
                        </span>
                        <span class="author-block">
                            <span lang="en"><a href="https://mingxuan.github.io/">Mingxuan Wang</a><sup>1,6</sup></span>
                            <span lang="zh"><a href="https://mingxuan.github.io/">ç‹æ˜è½©</a><sup>1,6</sup></span>
                        </span>
                    </div>
                    <div class="publication-affiliations">
                        <span class="affiliation-block">
                            <span lang="en"><sup>1</sup>ByteDance Seed</span>
                            <span lang="zh"><sup>1</sup>å­—èŠ‚è·³åŠ¨ Seed</span>
                        </span><br>
                        <span class="affiliation-block">
                            <span lang="en"><sup>3</sup>Institute for AI Industry Research (AIR), Tsinghua University</span>
                            <span lang="zh"><sup>3</sup>æ¸…åå¤§å­¦ æ™ºèƒ½äº§ä¸šç ”ç©¶é™¢ï¼ˆAIRï¼‰</span>
                        </span><br>
                        <span class="affiliation-block">
                            <span lang="en"><sup>6</sup>SIA-Lab of Tsinghua AIR and ByteDance Seed</span>
                            <span lang="zh"><sup>6</sup>æ¸…å-å­—èŠ‚è”åˆå®éªŒå®¤SIA-Lab</span>
                        </span>
                        <div class="affiliation-note">
                            <span lang="en">*Project Lead; â€ Equal Contribution</span>
                            <span lang="zh">*é¡¹ç›®è´Ÿè´£äºº; â€ åŒç­‰è´¡çŒ®</span>
                        </div>
                        <div class="affiliation-note">
                            <span lang="en">Contact: <a href="mailto:jiangjiec@bytedance.com">jiangjiec@bytedance.com</a></span>
                            <span lang="zh">è”ç³»æ–¹å¼: <a href="mailto:jiangjiec@bytedance.com">jiangjiec@bytedance.com</a></span>
                        </div>
                    </div>
                    <div class="publication-links">
                        <span class="link-block">
                            <a href="http://arxiv.org/abs/2505.19914" class="external-link button is-dark">
                                <span class="icon">
                                    <i class="fas fa-file-pdf"></i>
                                </span>
                                <span lang="en">Paper</span>
                                <span lang="zh">è®ºæ–‡</span>
                            </a>
                        </span>
                        <span class="link-block">
                            <a href="https://github.com/BytedTsinghua-SIA/Enigmata" class="external-link button is-dark">
                                <span class="icon">
                                    <i class="fas fa-database"></i>
                                </span>
                                <span lang="en">Code</span>
                                <span lang="zh">ä»£ç </span>
                            </a>
                        </span>
                        <span class="link-block">
                            <a href="https://huggingface.co/datasets/BytedTsinghua-SIA/Enigmata-Eval" class="external-link button is-dark">
                                <span class="icon">
                                    <i class="fas fa-database"></i>
                                </span>
                                <span lang="en">Evaluation</span>
                                <span lang="zh">è¯„ä¼°</span>
                            </a>
                        </span>
                        <span class="link-block">
                            <a href="https://huggingface.co/BytedTsinghua-SIA/Enigmata-Qwen2.5-32B" class="external-link button is-dark">
                                <span class="icon">
                                    ğŸ¤—
                                </span>
                                <span lang="en">Model</span>
                                <span lang="zh">æ¨¡å‹</span>
                            </a>
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </section>

<section class="section" id="introduction">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-10">
                <h2 class="title is-3">
                    <span lang="en">Introduction</span>
                    <span lang="zh">ä»‹ç»</span>
                </h2>
                <div class="content">

                    <p lang="en">
                        We present <strong>RL-Memory Agent</strong>, a novel framework for long-text processing that directly optimizes task performance through end-to-end reinforcement learning, without any architectural modifications to the base model.
                    </p>
                    <p lang="zh">
                        æˆ‘ä»¬æ¨å‡ºäº†<strong>RL-Memory Agent</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨æ–°çš„é•¿æ–‡æœ¬å¤„ç†æ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ ç›´æ¥ä¼˜åŒ–é•¿æ–‡æœ¬ä»»åŠ¡æ€§èƒ½ï¼Œè€Œæ— éœ€æ›´æ”¹åº•å±‚æ¨¡å‹æ¶æ„ã€‚
                    </p>
                    <p lang="en">
                        <strong>RL-Memory Agent</strong> achieves three major breakthroughs:
                    </p>

                    <p lang="zh">
                        <strong>RL-Memory Agent</strong> å®ç°äº†ä¸‰å¤§æ ¸å¿ƒçªç ´ï¼š
                    </p>
                    <div
                        style="background-color: #f8fafc; border-radius: 12px; border-left: 6px solid #3a76ed; padding: 1.2rem; margin-bottom: 1.5rem;">
                            <div style="display: flex; flex-direction: column; gap: 0.8rem;">
                                <div style="display: flex; align-items: flex-start; gap: 0.8rem;"> 
                                    <div
                                        style="background-color: #3a76ed; color: white; padding: 0.4rem; border-radius: 50%; font-size: 0.9rem; display: flex; align-items: center; justify-content: center; min-width: 2rem; height: 2rem;">
                                        1
                                    </div>
                                    <div>
                                        <p style="margin: 0; line-height: 1.4;" lang="en"><strong>Novel memory mechanism: </strong>
                                            It introduces a notebook-style workflow where the agent reads input in segments and efficiently updates memory using an overwrite strategy. This enables handling virtually unlimited-length inputs within a fixed context window, fundamentally breaking the hard length limits of Transformer models.</p>
                                        <p style="margin: 0; line-height: 1.4;" lang="zh"><strong>æ–°å‹è®°å¿†æœºåˆ¶ï¼š</strong>
                                            æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è®°äº‹æœ¬å¼çš„å·¥ä½œæµï¼ŒAgent ä»¥åˆ†æ®µæ–¹å¼è¯»å–æ–‡æœ¬ï¼Œå¹¶å€ŸåŠ©è¦†å†™ç­–ç•¥é«˜æ•ˆæ›´æ–°è®°å¿†ã€‚è¯¥è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å›ºå®šä¸Šä¸‹æ–‡çª—å£å†…å¤„ç†å‡ ä¹ä»»æ„é•¿åº¦çš„è¾“å…¥ï¼Œä»æ ¹æœ¬ä¸Šçªç ´äº†ä¼ ç»Ÿ Transformer æ¶æ„çš„çª—å£é•¿åº¦é™åˆ¶ã€‚</p>
                                    </div>
                                </div>
                        
                                <div style="display: flex; align-items: flex-start; gap: 0.8rem;">
                                    <div
                                        style="background-color: #3a76ed; color: white; padding: 0.4rem; border-radius: 50%; font-size: 0.9rem; display: flex; align-items: center; justify-content: center; min-width: 2rem; height: 2rem;">
                                        2
                                    </div>
                                    <div>
                                    <p style="margin: 0; line-height: 1.4;" lang="en"><strong>O(n) linear complexity:</strong> 
                                        The agent design decouples compute and memory cost from input length, reducing the processing burden from quadratic to linear with respect to input size.</p>
                                    <p style="margin: 0; line-height: 1.4;" lang="zh"><strong>O(n) çº¿æ€§å¤æ‚åº¦ï¼š</strong>
                                        è¿™ä¸€åˆ›æ–°æ€§çš„ Agent è®¾è®¡å°†è®¡ç®—ä¸æ˜¾å­˜å¼€é”€ä¸æ–‡æœ¬é•¿åº¦è§£è€¦ï¼Œä½¿å¾—å¤„ç†é•¿æ–‡æœ¬çš„å¤æ‚åº¦ç”±åŸæœ¬çš„äºŒæ¬¡æ–¹å¢é•¿è½¬å˜ä¸ºçº¿æ€§å¢é•¿ã€‚</p>
                                    </div>
                                </div>

                                <div style="display: flex; align-items: flex-start; gap: 0.8rem;">
                                    <div
                                        style="background-color: #3a76ed; color: white; padding: 0.4rem; border-radius: 50%; font-size: 0.9rem; display: flex; align-items: center; justify-content: center; min-width: 2rem; height: 2rem;">
                                        3
                                    </div>
                                    <div>
                                        <p style="margin: 0; line-height: 1.4;" lang="en"><strong>RL-driven extrapolation:</strong>
                                            A modified GRPO algorithm supports training on multi-turn generation tasks with independent contexts. As a result, models trained with 32K contexts can extrapolate to 3.5M-token sequences with <5% performance drop.</p>
                                        <p style="margin: 0; line-height: 1.4;" lang="zh"><strong>å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„å¤–æ¨èƒ½åŠ›:</strong>
                                            æˆ‘ä»¬æ”¹è¿›äº† GRPO ç®—æ³•ï¼Œä½¿å…¶æ”¯æŒç‹¬ç«‹ä¸Šä¸‹æ–‡çš„å¤šè½®ç”Ÿæˆè®­ç»ƒã€‚åŸºäºæ­¤ï¼Œè®­ç»ƒå‡ºçš„ Memory Agent åœ¨32Kä¸Šä¸‹æ–‡é•¿åº¦çš„æ•°æ®ä¸Šè®­ç»ƒåï¼Œèƒ½å¤Ÿæ— æŸå¤–æ¨è‡³3.5M tokençš„é—®ç­”ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½ä¸‹é™ä½äº5%ã€‚</p>
                                    </div>
                                </div>                                
                        </div>
                    </div>

                    <p lang="en">
                        In a simple yet effective way, we demonstrateâ€”for the first timeâ€”a trainable memory ability empowered by reinforcement learning. RL-Memory Agent showcases the untapped potential of optimizing specialized workflows via policy learning rather than architectural changes.
                    </p>
                    <p lang="zh">
                        æˆ‘ä»¬ä»¥ä¸€ç§ç®€æ´è€Œé«˜æ•ˆçš„æ–¹å¼ï¼Œé¦–æ¬¡å®ç°äº†çœŸæ­£æ„ä¹‰ä¸Šçš„ã€ç”±å¼ºåŒ–å­¦ä¹ èµ‹äºˆçš„å¯è®­ç»ƒè®°å¿†èƒ½åŠ›ï¼Œå……åˆ†å±•ç°äº†å¼ºåŒ–å­¦ä¹ åœ¨ä¼˜åŒ–ç‰¹å®šå·¥ä½œæµæ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚
                    </p>

                    <strong><em style="color: #3a76ed"><center lang="en">We believe RL-Memory Agent paves the way for the next generation of scalable long-context LLMs.</center></em></strong>
                    <strong><em style="color: #3a76ed"><center lang="zh">æˆ‘ä»¬ç›¸ä¿¡ï¼ŒRL-Memory Agent ä¸ºä¸‹ä¸€ä»£å¯æ‰©å±•çš„é•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹å¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚</center></em></strong>

                </div>
            </div>
        </div>
    </div>
</section>

<section class="section" id="method">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-10">
                <h2 class="title is-3">
                    <span lang="en">Method</span>
                    <span lang="zh">æ–¹æ³•ä»‹ç»</span>
                </h2>
                <div class="content">

                    <p lang="en">
                        Inspired by human strategies for long-text comprehension and neural memory models such as Neural Turing Machines and Memory Networks, we propose a novel architecture-free framework called <strong>Memory Agent</strong>, which equips LLMs with a dynamically updated memory module.
                    </p>
                    <p lang="zh">
                        å—äººç±»å¤„ç†é•¿æ–‡æœ¬æ—¶çš„è¡Œä¸ºæ¨¡å¼å¯å‘ï¼Œå¹¶å€Ÿé‰´äº†Neural Turing Machineå’ŒMemory Networkç­‰ç¥ç»ç½‘ç»œä¸è®°å¿†æœºåˆ¶ç»“åˆçš„å·¥ä½œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€ä¿®æ”¹æ¨¡å‹æ¶æ„çš„é•¿æ–‡å¤„ç†æ–°æ–¹æ¡ˆâ€”â€”<strong>Memory Agent</strong>ï¼Œå³â€œç»™LLMè£…ä¸ŠåŠ¨æ€æ›´æ–°çš„è®°å¿†æ¨¡å—â€ã€‚
                    </p>

                    <p lang="en">
                        <strong>Memory Agent</strong> introduces a fixed-size auxiliary memory panel that allows the model to read long texts in segments and update memory after each segment. This forms a new "local processing + global fusion" workflow. During inference, the memory is dynamically written and aggregated to generate the final output.
                    </p>
                    <p lang="zh">
                        <strong>Memory Agent</strong>å¼•å…¥äº†ä¸€ä¸ªå›ºå®šé•¿åº¦çš„è¾…åŠ©è®°å¿†é¢æ¿ï¼Œå…è®¸æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶ä»¥åˆ†æ®µæ–¹å¼è¯»å–è¾“å…¥ï¼Œå¹¶åœ¨æ¯ä¸€æ®µåä¸»åŠ¨æ›´æ–°è®°å¿†çŠ¶æ€ï¼Œä»è€Œå®ç°â€œå±€éƒ¨å¤„ç† + å…¨å±€èåˆâ€çš„æ–°å‹å·¥ä½œæµã€‚è¯¥è®°å¿†æ¨¡å—åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸æ–­æ›´æ–°ï¼Œå¹¶åœ¨æ‰€æœ‰æ®µè½å¤„ç†å®Œæ¯•åèšåˆå…³é”®ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆè¾“å‡ºã€‚
                    </p>

                    <figure>
                        <img src="path_to_image_1.png" alt="Memory Agent Architecture Overview">
                    </figure>

                    <p lang="en">
                        <strong>Traditional Modeling:</strong> Conventional autoregressive LMs model the joint probability of a length-N token sequence as:
                        <br>
                        $$p(\mathbf{x}_{1:N})=p(x_1)\prod_{n=2}^N p(x_n \mid \mathbf{x}_{1:n-1})$$
                        This requires caching all previous tokens, leading to quadratic complexity.
                    </p>
                    <p lang="zh">
                        <strong>ä¼ ç»Ÿå»ºæ¨¡æ–¹å¼ï¼š</strong> ä¼ ç»Ÿè‡ªå›å½’è¯­è¨€æ¨¡å‹é€šè¿‡ä»¥ä¸‹æ–¹å¼å»ºæ¨¡é•¿åº¦ä¸ºNçš„tokenåºåˆ—ï¼š
                        <br>
                        $$p(\mathbf{x}_{1:N})=p(x_1)\prod_{n=2}^N p(x_n \mid \mathbf{x}_{1:n-1})$$
                        è¿™ç§æ–¹å¼éœ€è¦ç¼“å­˜å…¨éƒ¨å†å²tokenï¼Œå¯¼è‡´è®¡ç®—ä¸æ˜¾å­˜å¼€é”€å‘ˆäºŒæ¬¡æ–¹å¢é•¿ã€‚
                    </p>

                    <p lang="en">
                        <strong>Memory Agent Process:</strong> Input sequence is divided into K chunks $$(c^1, c^2, ..., c^K)$$, each with â‰¤C tokens. We introduce latent memory states $$m^{1:K-1}$$ with $$m^0 = âˆ…$$ and reformulate the generation as:
                        <br>
                        $$p(\mathbf{x}_{1:N}) = \sum_{\mathbf{m}^{1:K-1}} \prod_{k=1}^{K}
                        \underbrace{p(\mathbf{c}^k \mid \mathbf{m}^{k-1})}_{\text{read}} \cdot
                        \underbrace{p(\mathbf{m}^k \mid \mathbf{c}^k, \mathbf{m}^{k-1})}_{\text{write}}$$
                    </p>
                    <p lang="zh">
                        <strong>Memory Agent æ ¸å¿ƒæµç¨‹ï¼š</strong> å°†è¾“å…¥åºåˆ—åˆ†ä¸º K ä¸ªå— $$(c^1, c^2, ..., c^K)$$ï¼Œæ¯å—æœ€å¤šåŒ…å« $$C$$ ä¸ªtokenã€‚æˆ‘ä»¬å¼•å…¥æ½œåœ¨çš„è®°å¿†å˜é‡ $$m^{1:K-1}$$ å’Œåˆå§‹çŠ¶æ€ $$m^0 = âˆ…$$ï¼Œé‡æ–°å»ºæ¨¡ç”Ÿæˆè¿‡ç¨‹ä¸ºï¼š
                        <br>
                        $$p(\mathbf{x}_{1:N}) = \sum_{\mathbf{m}^{1:K-1}} \prod_{k=1}^{K}
                        \underbrace{p(\mathbf{c}^k \mid \mathbf{m}^{k-1})}_{\text{è¯»å–}} \cdot
                        \underbrace{p(\mathbf{m}^k \mid \mathbf{c}^k, \mathbf{m}^{k-1})}_{\text{å†™å…¥}}$$
                    </p>

                    <h4 lang="en"><strong>Reinforcement Learning Optimization</strong></h4>
                    <h4 lang="zh"><strong>å¼ºåŒ–å­¦ä¹ è®­ç»ƒæœºåˆ¶</strong></h4>

                    <p lang="en">
                        Memory Agent faces two challenges during training: 1) Non-differentiability of memory operations hinders standard backpropagation; 2) The memory sequence is latent and lacks explicit supervision. We adopt reinforcement learning to optimize memory control.
                    </p>
                    <p lang="zh">
                        Memory Agentåœ¨è®­ç»ƒæ—¶é¢ä¸´ä¸¤ä¸ªæŒ‘æˆ˜ï¼š1ï¼‰æ— æ³•é€šè¿‡æ ‡å‡†æ¢¯åº¦ä¼˜åŒ–è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼›2ï¼‰å¼•å…¥çš„è®°å¿†åºåˆ—æ˜¯æ½œåœ¨å˜é‡ï¼Œç¼ºä¹æ˜¾å¼ç›‘ç£ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ–¹å¼è®­ç»ƒæ¨¡å‹çš„è®°å¿†èƒ½åŠ›ã€‚
                    </p>

                    <figure>
                        <img src="path_to_image_2.png" alt="RL Optimization Overview">
                    </figure>

                    <h4 lang="en"><strong>Multi-Turn Dialog RL Training</strong></h4>
                    <h4 lang="zh"><strong>å¤šè½®å¯¹è¯å¼ºåŒ–å­¦ä¹ æœºåˆ¶</strong></h4>

                    <p lang="en">
                        For each sample, multiple independent dialog sequences are generated in parallel. The final answer is selected to compute a reward, which is then distributed intelligently across all related dialogues. Chunk-level token losses are computed under a 3D optimization structure: (group, turn, token).
                    </p>
                    <p lang="zh">
                        åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªè¾“å…¥æ ·æœ¬ä¼šå¹¶è¡Œç”Ÿæˆå¤šç»„ç‹¬ç«‹çš„å¯¹è¯åºåˆ—ã€‚ç³»ç»Ÿæ ¹æ®æœ€ç»ˆçš„å›ç­”ç¡®å®šå¥–åŠ±å€¼ï¼Œå¹¶é€šè¿‡ç»„å½’ä¸€åŒ–æ–¹å¼å°†å…¶æ™ºèƒ½åˆ†é…åˆ°æ‰€æœ‰ç›¸å…³å¯¹è¯ä¸­ã€‚æŸå¤±ä»¥chunkä¸ºå•ä½è®¡ç®—ï¼Œä¼˜åŒ–ç»“æ„åŒ…å«â€œç»„åˆ«-å¯¹è¯è½®æ¬¡-tokenâ€ä¸‰ç»´ã€‚
                    </p>

                    <figure>
                        <img src="path_to_image_3.png" alt="Multi-turn Optimization Scheme">
                    </figure>

                </div>
            </div>
        </div>
    </div>
</section>

    
    <section class="section" id="eval">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-10">
                    <h2 class="title is-3">
                        <span lang="en">âš–ï¸ Enigmata-Eval: Evaluating Logical Reasoning Capabilities</span>
                        <span lang="zh">âš–ï¸ Enigmata-Eval: è¯„ä¼°é€»è¾‘æ¨ç†èƒ½åŠ›</span>
                    </h2>
                    <div class="content">
                        <p lang="en">Enigmata-Eval is a comprehensive benchmark containing 4,758 puzzle instances across Easy, Medium, and Hard difficulty levels. Each task provides 50 instances per difficulty level where possible, with strict train-eval separation to prevent data leakage.</p>
                        <p lang="zh">Enigmata-Evalæ˜¯ä¸€ä¸ªç»¼åˆåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«4,758ä¸ªè·¨è¶Šç®€å•ã€ä¸­ç­‰å’Œå›°éš¾éš¾åº¦çº§åˆ«çš„è°œé¢˜å®ä¾‹ã€‚æ¯ä¸ªä»»åŠ¡å°½å¯èƒ½æä¾›æ¯ä¸ªéš¾åº¦çº§åˆ«50ä¸ªå®ä¾‹ï¼Œå¹¶ä¸¥æ ¼åˆ†ç¦»è®­ç»ƒå’Œè¯„ä¼°æ•°æ®ä»¥é˜²æ­¢æ•°æ®æ³„æ¼ã€‚</p>

                        <p lang="en">ğŸ“¥ <strong>Download Enigmata-Eval</strong>: <a href="https://huggingface.co/datasets/BytedTsinghua-SIA/Enigmata-Eval">HuggingFace Dataset</a></p>
                        <p lang="zh">ğŸ“¥ <strong>ä¸‹è½½Enigmata-Eval</strong>: <a href="https://huggingface.co/datasets/BytedTsinghua-SIA/Enigmata-Eval">HuggingFaceæ•°æ®é›†</a></p>
                    
         

                
                
                
                </div>
            </div>
        </div>
    </section>
    
    <section class="section" id="model">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-10">
                    <h2 class="title is-3">
                        <span lang="en">ğŸ¤– Enigmata-Model: The Training Recipe</span>
                        <span lang="zh">ğŸ¤– Enigmata-Model: è®­ç»ƒæ–¹æ³•</span>
                    </h2>
                    <div class="content">
                        <p lang="en">Our training methodology follows a two-stage process designed to systematically build reasoning abilities: 
                            (1) rejection fine-tuning to establish foundational reasoning patterns, and 
                            (2) multi-task RL to develop general reasoning skills that transfer across diverse problem domains.</p>
                        <p lang="zh">æˆ‘ä»¬çš„è®­ç»ƒæ–¹æ³•éµå¾ªä¸¤é˜¶æ®µè¿‡ç¨‹ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°æ„å»ºæ¨ç†èƒ½åŠ›ï¼š
                            (1) æ‹’ç»å¼å¾®è°ƒä»¥å»ºç«‹åŸºç¡€æ¨ç†æ¨¡å¼ï¼Œä»¥åŠ
                            (2) å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ï¼Œä»¥å‘å±•å¯åœ¨ä¸åŒé—®é¢˜é¢†åŸŸä¹‹é—´è¿ç§»çš„é€šç”¨æ¨ç†æŠ€èƒ½ã€‚</p>
                    </div>
                    <div class="content">
                        <div
                        style="background-color: #f8fafc; border-radius: 12px; border-left: 6px solid #3a76ed; padding: 1.2rem; margin-bottom: 1.5rem;">
                            <div style="display: flex; flex-direction: column; gap: 0.8rem;">
                                <div style="display: flex; align-items: flex-start; gap: 0.8rem;"> 
                                    <div
                                        style="background-color: #3a76ed; color: white; padding: 0.4rem; border-radius: 50%; font-size: 0.9rem; display: flex; align-items: center; justify-content: center; min-width: 2rem; height: 2rem;">
                                        1
                                    </div>
                                    <div>
                                        <p style="margin: 0; line-height: 1.4;" lang="en"><strong>Rejection Fine-tuning:</strong>
                                            This initial stage focuses on building foundational reasoning by fine-tuning the model with high-quality solutions from a balanced mix of math and puzzle problems, including ARC-AGI.</p>
                                        <p style="margin: 0; line-height: 1.4;" lang="zh"><strong>æ‹’ç»å¼å¾®è°ƒ:</strong>
                                            è¿™ä¸€åˆå§‹é˜¶æ®µä¸“æ³¨äºé€šè¿‡ä½¿ç”¨æ¥è‡ªæ•°å­¦å’Œè°œé¢˜é—®é¢˜ï¼ˆåŒ…æ‹¬ARC-AGIï¼‰çš„å¹³è¡¡æ··åˆçš„é«˜è´¨é‡è§£å†³æ–¹æ¡ˆå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»è€Œå»ºç«‹åŸºç¡€æ¨ç†èƒ½åŠ›ã€‚</p>
                                    </div>
                                </div>
                        
                                <div style="display: flex; align-items: flex-start; gap: 0.8rem;">
                                    <div
                                        style="background-color: #3a76ed; color: white; padding: 0.4rem; border-radius: 50%; font-size: 0.9rem; display: flex; align-items: center; justify-content: center; min-width: 2rem; height: 2rem;">
                                        2
                                    </div>
                                    <div>
                                    <p style="margin: 0; line-height: 1.4;" lang="en"><strong>RL with Verifiable Puzzles:</strong> The model then undergoes reinforcement learning using VC-PPO, where an automated verifier for puzzles provides immediate rewards, enabling an automatic RL pipeline for puzzle reasoning.</p>
                                    <p style="margin: 0; line-height: 1.4;" lang="zh"><strong>å¯éªŒè¯è°œé¢˜çš„å¼ºåŒ–å­¦ä¹ :</strong> ç„¶åï¼Œæ¨¡å‹ä½¿ç”¨VC-PPOè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œå…¶ä¸­è°œé¢˜çš„è‡ªåŠ¨éªŒè¯å™¨æä¾›å³æ—¶å¥–åŠ±ï¼Œä»è€Œä¸ºè°œé¢˜æ¨ç†å¯ç”¨è‡ªåŠ¨å¼ºåŒ–å­¦ä¹ æµç¨‹ã€‚</p>
                                    </div>
                                </div>

                                <div style="display: flex; align-items: flex-start; gap: 0.8rem;">
                                    <div
                                        style="background-color: #3a76ed; color: white; padding: 0.4rem; border-radius: 50%; font-size: 0.9rem; display: flex; align-items: center; justify-content: center; min-width: 2rem; height: 2rem;">
                                        3
                                    </div>
                                    <div>
                                        <p style="margin: 0; line-height: 1.4;" lang="en"><strong>Multi-task Training:</strong> To develop general and transferable logical reasoning, the training incorporates multi-task methods like Mix-training RL and Multi-stage RL, combining diverse puzzle types (Enigmata, ARC-AGI) with challenging mathematical problems (AIME) while maintaining a balanced ratio.</p>
                                        <p style="margin: 0; line-height: 1.4;" lang="zh"><strong>å¤šä»»åŠ¡è®­ç»ƒ:</strong> ä¸ºäº†å‘å±•é€šç”¨å’Œå¯è¿ç§»çš„é€»è¾‘æ¨ç†ï¼Œè®­ç»ƒç»“åˆäº†å¤šä»»åŠ¡æ–¹æ³•ï¼Œå¦‚æ··åˆè®­ç»ƒå¼ºåŒ–å­¦ä¹ å’Œå¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ ï¼Œå°†å„ç§è°œé¢˜ç±»å‹ï¼ˆEnigmataã€ARC-AGIï¼‰ä¸å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦é—®é¢˜ï¼ˆAIMEï¼‰ç»“åˆèµ·æ¥ï¼ŒåŒæ—¶ä¿æŒå¹³è¡¡æ¯”ä¾‹ã€‚</p>
                                    </div>
                                </div>                                
                        </div>
                    </div>
                    </div>

                    
                    <div class="content">
                        <h4>
                            <span lang="en">ğŸ‘€ Experimental Results</span>
                            <span lang="zh">ğŸ‘€ å®éªŒç»“æœ</span>
                        </h4>
                        <div class="content">
                            <p lang="en">Our model, specifically the 32B parameter version, significantly outperforms most public models on Enigmata-Eval and ARC-AGI, showcasing enhanced general logical reasoning. This success stems from effective rejection fine-tuning (RFT) and multi-task RL strategies, which improve generalization while preserving existing math reasoning abilities.</p>
                            <p lang="zh">æˆ‘ä»¬çš„æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯32Bå‚æ•°ç‰ˆæœ¬ï¼Œåœ¨Enigmata-Evalå’ŒARC-AGIä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºå¤§å¤šæ•°å…¬å¼€æ¨¡å‹ï¼Œå±•ç¤ºäº†å¢å¼ºçš„é€šç”¨é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚è¿™ä¸€æˆåŠŸæºäºæœ‰æ•ˆçš„æ‹’ç»å¼å¾®è°ƒï¼ˆRFTï¼‰å’Œå¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œå®ƒä»¬åœ¨ä¿ç•™ç°æœ‰æ•°å­¦æ¨ç†èƒ½åŠ›çš„åŒæ—¶æé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚</p>
                        </div>
                        <figure>
                            <img src="./assets/main_results_1.png" alt="A descriptive alt text for your image" style="width: 600px;">
                            <figcaption style="color: gray;">
                                <span lang="en">Performance of reasoning, generic, and our trained LLMs on reasoning benchmarks</span>
                                <span lang="zh">æ¨ç†å‹ã€é€šç”¨å‹å’Œæˆ‘ä»¬è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°</span>
                            </figcaption>
                          </figure>
                        
                          <div class="content">
                            <p lang="en">On Enigmata-Eval, our Qwen2.5-32B-Enigmata model excels in Crypto, Arithmetic, and Logic tasks, indicating strong rule-based reasoning. It also performs competitively in search tasks, though spatial and sequential categories remain challenging.</p>
                            <p lang="zh">åœ¨Enigmata-Evalä¸Šï¼Œæˆ‘ä»¬çš„Qwen2.5-32B-Enigmataæ¨¡å‹åœ¨åŠ å¯†ã€ç®—æœ¯å’Œé€»è¾‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¡¨æ˜å…¶å…·æœ‰å¼ºå¤§çš„åŸºäºè§„åˆ™çš„æ¨ç†èƒ½åŠ›ã€‚å®ƒåœ¨æœç´¢ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å¾—å¾ˆæœ‰ç«äº‰åŠ›ï¼Œå°½ç®¡ç©ºé—´å’Œåºåˆ—ç±»åˆ«ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</p>
                        </div>

                          <figure>
                            <img src="./assets/main_results2.png" alt="A descriptive alt text for your image" style="width: 600px;">
                            <figcaption style="color: gray;">
                                <span lang="en">Performance of reasoning LLMs, generic LLMs, and our trained LLMs on Enigmata-Eval</span>
                                <span lang="zh">æ¨ç†å‹å¤§è¯­è¨€æ¨¡å‹ã€é€šç”¨å‹å¤§è¯­è¨€æ¨¡å‹å’Œæˆ‘ä»¬è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹åœ¨Enigmata-Evalä¸Šçš„è¡¨ç°</span>
                            </figcaption>
                          </figure>

                    </div>

                    
                    <div class="content">
                        <h4>
                            <span lang="en">ğŸŒŸ Generalization with Scaling: Free Lunch from Enigmata</span>
                            <span lang="zh">ğŸŒŸ éšè§„æ¨¡æ‰©å±•çš„æ³›åŒ–èƒ½åŠ›ï¼šEnigmataçš„å…è´¹åˆé¤</span>
                        </h4>
                    <div class="content">
                        <p lang="en">Incorporating the <strong>Enigmata-Data</strong> synthetic puzzle dataset into large-scale model training, e.g., <a href="https://arxiv.org/abs/2504.13914">Seed1.5-Thinking</a>, surprisingly improving performance on challenging benchmarks like AIME and GPQA Diamond. This demonstrates an unexpected <strong>generalization benefit</strong> for advanced reasoning models.</p>
                        <p lang="zh">å°†<strong>Enigmata-Data</strong>åˆæˆè°œé¢˜æ•°æ®é›†çº³å…¥å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒï¼Œä¾‹å¦‚<a href="https://arxiv.org/abs/2504.13914">Seed1.5-Thinking</a>ï¼Œä»¤äººæƒŠè®¶åœ°æé«˜äº†åœ¨AIMEå’ŒGPQA Diamondç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ã€‚è¿™å±•ç¤ºäº†é«˜çº§æ¨ç†æ¨¡å‹çš„æ„å¤–<strong>æ³›åŒ–ä¼˜åŠ¿</strong>ã€‚</p>
                    </div>

                      <figure>
                        <img src="./assets/seed_thinking.png" alt="A descriptive alt text for your image" style="width: 600px;">
                        <figcaption style="color: gray;">Results on benchmarks for general reasoning capabilities</figcaption>
                      </figure>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <!-- <section class="section">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-10">
                    <h2 class="title is-3">Performance Results</h2>
                    <div class="content">
                        
                       
                            
                        <div class="content">
                            <p>Our model, specifically the 32B parameter version, significantly outperforms most public models on Enigmata-Eval and ARC-AGI, showcasing enhanced general logical reasoning. This success stems from effective rejection fine-tuning (RFT) and multi-task RL strategies, which improve generalization while preserving existing math reasoning abilities.</p>
                        </div>
                        <figure>
                            <img src="./assets/main_results_1.png" alt="A descriptive alt text for your image" style="width: 600px;">
                            <figcaption style="color: gray;">Performance of reasoning, generic, and our trained LLMs on reasoning benchmarks</figcaption>
                          </figure>
                        
                          <div class="content">
                            <p>On Enigmata-Eval, our Qwen2.5-32B-Enigmata model excels in Crypto, Arithmetic, and Logic tasks, indicating strong rule-based reasoning. It also performs competitively in search tasks, though spatial and sequential categories remain challenging.</p>
                        </div>

                          <figure>
                            <img src="./assets/main_results2.png" alt="A descriptive alt text for your image" style="width: 600px;">
                            <figcaption style="color: gray;">Performance of reasoning LLMs, generic LLMs, and our trained LLMs on Enigmata-Eval</figcaption>
                          </figure>

                          <div class="content">
                            <p>Incorporating the <strong>Enigmata-Data</strong> synthetic puzzle dataset into large-scale model training significantly boosts general reasoning, including math and STEM, surprisingly improving performance on challenging benchmarks like AIME and GPQA Diamond. This demonstrates an unexpected <strong>generalization benefit</strong> for advanced reasoning models.</p>
                        </div>

                          <figure>
                            <img src="./assets/seed_thinking.png" alt="A descriptive alt text for your image" style="width: 600px;">
                            <figcaption style="color: gray;">
                                <span lang="en">Results on benchmarks for general reasoning capabilities</span>
                                <span lang="zh">é€šç”¨æ¨ç†èƒ½åŠ›åŸºå‡†æµ‹è¯•çš„ç»“æœ</span>
                            </figcaption>
                          </figure>


                    </div>
                </div>
            </div>
        </div>
    </section> -->

    <section class="section" id="citation">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-10">
                    <h2 class="title is-3">
                        <span lang="en">ğŸ“ Citation</span>
                        <span lang="zh">ğŸ“ å¼•ç”¨</span>
                    </h2>
                    <div class="content">
                        <p lang="en">If you find this work useful, please cite our paper:</p>
                        <p lang="zh">å¦‚æœæ‚¨å‘ç°è¿™é¡¹å·¥ä½œæœ‰ç”¨ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡ï¼š</p>
                        <pre><code class="latex">
@article{chen2025enigmata,
    title={Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles},
    author={Chen, Jiangjie and He, Qianyu and Yuan, Siyu and Chen, Aili and Cai, Zhicheng and Dai, Weinan and Yu, Hongli and Yu, Qiying and Li, Xuefeng and Chen, Jiaze and others},
    journal={arXiv preprint arXiv:2505.19914},
    year={2025}
}
                        </code></pre>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <div class="has-text-centered">
                <p>
                    <span lang="en">Â© 2025 <a href="https://seed.bytedance.com">ByteDance Seed</a>. Modified from <a href="https://github.com/seed-enigmata/seed-enigmata.github.io">R2E-Gym</a>.</span>
                    <span lang="zh">Â© 2025 <a href="https://seed.bytedance.com">å­—èŠ‚è·³åŠ¨Seed</a>. ä¿®æ”¹è‡ª <a href="https://github.com/seed-enigmata/seed-enigmata.github.io">R2E-Gym</a>.</span>
                </p>
            </div>
        </div>
    </footer>


</body>


</html>
